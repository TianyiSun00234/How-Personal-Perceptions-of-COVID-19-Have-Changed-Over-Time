{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nimport itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('fivethirtyeight')\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df4 = pd.read_csv('/kaggle/input/testset/april_comments.csv')\ndf3 = pd.read_csv('/kaggle/input/testset/march_comments.csv')\ndf1 = pd.read_csv('/kaggle/input/testset/january_comments.csv')\ndf2 = pd.read_csv('/kaggle/input/testset/february_comments.csv')\ntopics = pd.read_csv('/kaggle/input/topic907/topics.csv')\n#sentiments = pd.read_csv('/kaggle/input/topic907/sentiment912.csv')\n\nframe = pd.concat([df1, df2, df3, df4], ignore_index=True)\nframe[['date','author','preview']]\n\nresult = pd.merge(frame[['date','author','preview']],\n                   topics[['topic1']],\n                   how = 'inner',\n                   left_index=True, right_index=True)\n\nresult.rename(columns={ \"topic1\": \"topics\"}, inplace=True)\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.index.name = 'id'\nresult['date']= result['date'].astype('datetime64[ns]')\ndf = result\n\ntopic1 = df.loc[df['topics'] == 1]\ntopic1_num = topic1.groupby('date')['topics'].count().reset_index()\ntopic1_num.rename(columns ={'topics': 'topic1_num'}, inplace=True )\n\ntopic2 = df.loc[df['topics'] ==2]\ntopic2_num = topic2.groupby('date')['topics'].count().reset_index()\ntopic2_num.rename(columns ={'topics': 'topic2_num'}, inplace=True )\ntopic2_num\n\ntopic3 = df.loc[df['topics'] ==3]\ntopic3_num = topic3.groupby('date')['topics'].count().reset_index()\ntopic3_num.rename(columns ={'topics': 'topic3_num'}, inplace=True )\ntopic3_num\n\ntopic4 = df.loc[df['topics'] ==4]\ntopic4_num = topic4.groupby('date')['topics'].count().reset_index()\ntopic4_num.rename(columns ={'topics': 'topic4_num'}, inplace=True )\ntopic4_num\n\ntopic5 = df.loc[df['topics'] == 5]\ntopic5_num = topic5.groupby('date')['topics'].count().reset_index() \ntopic5_num.rename(columns ={'topics': 'topic5_num'}, inplace=True )\n\ntopic12 = pd.merge(topic1_num,\n                topic2_num,\n                how = 'outer',\n                on = 'date')\n\ntopic123 = pd.merge(topic12,\n                topic3_num,\n                how = 'outer',\n                on = 'date')\n\ntopic1234 = pd.merge(topic123,\n                topic4_num,\n                how = 'outer',\n                on = 'date')\n\ntopic12345 = pd.merge(topic1234,\n                topic5_num,\n                how = 'outer',\n                on = 'date')\n\ntopic12345['topic5_num'] = topic12345['topic5_num'].fillna(0.0)\ntopic12345['topic1_num'] = topic12345['topic1_num'].fillna(0.0)\ntopic12345['topic2_num'] = topic12345['topic2_num'].fillna(0.0)\ntopic12345['topic3_num'] = topic12345['topic3_num'].fillna(0.0)\ntopic12345['topic4_num'] = topic12345['topic4_num'].fillna(0.0)\ntopic12345.rename(columns ={'topic1_num':'topic1','topic2_num':'topic2','topic3_num':'topic3','topic4_num':'topic4','topic5_num':'topic5'}, inplace = True)\ntopic12345 = topic12345.sort_values(by='date',ascending=True)\ntopic12345","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic12345.to_csv('counttopics913.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n# A tutorial of machine learning mastery\n## ARIMA (08-25)\n[https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/](http://)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\n \ndef parser(x):\n    return datetime.strptime( x, '%Y-%m-%d')\n \nseries = read_csv('/kaggle/working/counttopics913.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\nprint(series.head())\nseries.plot()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\nfrom pandas.plotting import autocorrelation_plot\n \ndef parser(x):\n    return datetime.strptime(x, '%Y-%m-%d')\n \nseries = read_csv('/kaggle/working/counttopics913.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\nautocorrelation_plot(series)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import datetime\nfrom pandas import DataFrame\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib import pyplot\n \ndef parser(x):\n    return datetime.strptime(x, '%Y-%m-%d')\n \nseries = read_csv('/kaggle/working/counttopics913.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n# fit model\nmodel = ARIMA(series, order=(1,1,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())\n# plot residual errors\nresiduals = DataFrame(model_fit.resid)\nresiduals.plot()\npyplot.show()\nresiduals.plot(kind='kde')\npyplot.show()\nprint(residuals.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom pandas import datetime\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\ndef parser(x):\n    return datetime.strptime(x, '%Y-%m-%d')\n\nseries = read_csv('/kaggle/working/counttopics913.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\nX = series.values\nsize = int(len(X) * 0.66)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n    model = ARIMA(history, order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\n# plot\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----\n## modelling"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install livelossplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout, Flatten,LSTM,RepeatVector,TimeDistributed,Conv1D,MaxPooling1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom livelossplot.keras import PlotLossesCallback\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = topic1_num\ndf.rename(columns={'topic1_num': 'x'}, inplace=True) \ndf = df.drop(['date'],axis = 1)\n\n#df = topic1_num\n#df.rename(columns={'topic1_num': 'x'}, inplace=True) \n#df = df.drop(['date'],axis = 1)\n\n#df = topic2_num\n#df.rename(columns={'topic2_num': 'x'}, inplace=True) \n#df = df.drop(['date'],axis = 1)\n\n#df = topic3_num\n#df.rename(columns={'topic3_num': 'x'}, inplace=True) \n#df = df.drop(['date'],axis = 1)\n\n#df = topic4_num\n#df.rename(columns={'topic4_num': 'x'}, inplace=True) \n#df = df.drop(['date'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## use the past days to forecast the following.\ndef train_test_builder(df,size_test=24,length_train_sequence = 24,length_forecast = 12, normalize = False):\n    \n    train_df = df[:-size_test]    \n    \n    # create training sets\n    train_x, train_y = window_splitter(train_df)\n    \n    # create test set\n    tmp = df[:length_forecast]\n    tmp = pd.concat([df,tmp],ignore_index = True)\n    tmp[-length_forecast:] = np.nan  # unknown\n    test_x,test_y = window_splitter(tmp)\n    \n    # drop elements in training\n    test_x = test_x[train_x.shape[0]:]\n    test_y = test_y[train_x.shape[0]:]\n    for i in range(length_forecast):\n        test_y[i,:(11-i)]=np.nan # present in training_set\n    \n    # normalize\n    if normalize:\n        m = train_df.x.mean()\n        sd = train_df.x.std()\n        train_x -= m\n        train_x/= sd\n        test_x -=m\n        test_x /= sd\n    \n    return train_x,train_y,test_x,test_y\n\n# iterate over the time steps and divide the data into overlapping windows; \ndef window_splitter(train_df,length_train_sequence = 24,length_forecast = 12):\n    i = 0\n    x,y = [],[]\n    while i + length_train_sequence+ length_forecast < len(train_df):\n        x.append(train_df.x[i:(i+length_train_sequence)].values)\n        y.append(train_df.x[(i+length_train_sequence):(i+length_train_sequence+length_forecast)].values)\n        i+=1\n\n    x = np.array(x).reshape(-1,length_train_sequence,1)\n    y = np.array(y).reshape(-1,length_forecast)\n    \n    return x,y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# a naive seasonal decomposition\nseries = df.x.values\nresult = seasonal_decompose(series, model='additive',freq=12,two_sided = False,)\nplt.plot(result.trend)\nplt.plot(result.seasonal)\nplt.plot(result.resid)\nplt.plot(result.observed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"deseasonalised_df = pd.DataFrame({'x':result.trend[12:]+result.resid[12:]})\ndeseasonalised_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build both versions\ndes_train_x,des_train_y,des_test_x, _ = train_test_builder(deseasonalised_df)\ntrain_x,train_y,test_x,test_y = train_test_builder(df)\n\ntrain_x.shape, train_y.shape, test_x.shape, test_y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluation function\ndef evaluate_predictions(pred_y, test_y):\n    return pd.DataFrame(abs(test_y-pred_y)).mean(skipna = True)\n\n\ndef plot_evaluation(pred_y,test_y,n=12):\n    scores = evaluate_predictions(pred_y,test_y)\n\n    print(\"Mean absolute error test set:\",scores.mean())\n\n    plt.figure(figsize=(6,4))\n    plt.plot(np.arange(1,13),scores)\n    plt.xticks(np.arange(1,13))\n    plt.xlabel(\"horizon [months]\", size = 15)\n    plt.ylabel(\"MAE\", size = 15)\n    plt.title(\"Scores LSTM on test set\")\n    plt.show()\n\n    plt.figure(figsize=(6,4))\n    plt.title(\"LSTM forecasting - test set window\")\n    plt.plot(np.arange(1,13),pred_y[n:(n+1)].reshape(-1,1),label = \"predictions\")\n    plt.plot(np.arange(1,13),test_y[n:(n+1)].reshape(-1,1),label = \"true values\")\n    plt.xticks(np.arange(1,13))\n    plt.xlabel(\"horizon [months]\", size = 15)\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n\n# define model\nmodel = Sequential()\nmodel.add(LSTM(256, activation='relu', input_shape=(None, 1)))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(n_outputs))\n\n# compile\nmodel.compile(loss='mse', optimizer= Adam(lr=1e-3), metrics = [\"mae\"])\n\n# callbacks\nearly_stopping = EarlyStopping(patience=32, monitor='val_loss', mode='auto', restore_best_weights=True)\ncallbacks=[PlotLossesCallback(), early_stopping]\n\n# fit network\nmodel.fit(des_train_x, des_train_y, \n          validation_split = 0.2, \n          epochs=100, \n          shuffle = True,\n          batch_size=16,\n          verbose=1,\n       #   callbacks = callbacks\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test set\npred_y = model.predict(des_test_x)\n# add seasonality of the past year\nfor i in np.arange(pred_y.shape[0]-1,-1,-1):\n    pred_y[pred_y.shape[0]-1-i] += result.seasonal[-(13+i):-(1+i)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_evaluation(pred_y,test_y,n=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# store predictions on test set\ntmp = pd.DataFrame(test_y-pred_y)\ntmp.columns = np.arange(1,13)\n\ntmp.to_csv(\"lstm_predictions.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------\n## Encoder Decoder \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n\ndes_train_y = des_train_y.reshape((des_train_y.shape[0], des_train_y.shape[1], 1))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(128, activation='relu', input_shape=(n_timesteps, n_features)))\nmodel.add(Dropout(0.1))\nmodel.add(RepeatVector(n_outputs))\nmodel.add(LSTM(128, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.1))\nmodel.add(TimeDistributed(Dense(64, activation='relu')))\nmodel.add(Dropout(0.1))\nmodel.add(TimeDistributed(Dense(1)))\n\n# compile\nmodel.compile(loss='mse', optimizer= Adam(lr=1e-3), metrics = [\"mae\"])\n\n# callbacks\nearly_stopping = EarlyStopping(patience=16, monitor='val_loss', mode='auto', restore_best_weights=True)\ncallbacks=[PlotLossesCallback(), early_stopping]\n\n\n# fit network\nmodel.fit(des_train_x, des_train_y, \n          validation_split = 0.1, \n          epochs=100, \n          shuffle = True,\n          batch_size=16,\n          verbose=1,\n     #     callbacks = callbacks,\n         )\n\n\n# predict test set\npred_y = model.predict(des_test_x).reshape(-1,12)\n# add seasonality of the past year\nfor i in np.arange(pred_y.shape[0]-1,-1,-1):\n    pred_y[pred_y.shape[0]-1-i] += result.seasonal[-(13+i):-(1+i)]\n\nplot_evaluation(pred_y,test_y,n=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## Encoder Decoder CNN LSTM "},{"metadata":{"trusted":true},"cell_type":"code","source":"n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n\ndes_train_y = des_train_y.reshape((des_train_y.shape[0], des_train_y.shape[1], 1))\n# define model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\nmodel.add(Dropout(0.1))\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(RepeatVector(n_outputs))\nmodel.add(LSTM(128, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.1))\nmodel.add(TimeDistributed(Dense(64, activation='relu')))\nmodel.add(TimeDistributed(Dense(1)))\n# compile\nmodel.compile(loss='mse', optimizer= Adam(lr=1e-3), metrics = [\"mae\"])\n\n# callbacks\nearly_stopping = EarlyStopping(patience=16, monitor='val_loss', mode='auto', restore_best_weights=True)\ncallbacks=[PlotLossesCallback(), early_stopping]\n\n# fit network\nmodel.fit(des_train_x, des_train_y, \n          validation_split = 0.1, \n          epochs=100, \n          shuffle = True,\n          batch_size=16,\n          verbose=1,\n        #  callbacks = callbacks,\n         )\n\n# predict test set\npred_y = model.predict(des_test_x).reshape(-1,12)\n# add seasonality of the past year\nfor i in np.arange(pred_y.shape[0]-1,-1,-1):\n    pred_y[pred_y.shape[0]-1-i] += result.seasonal[-(13+i):-(1+i)]\n\nplot_evaluation(pred_y,test_y,n=12)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n# A tutorial of machine learning mastery\n## LSTM Network for Regression (08-24)\n\n### [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)\nUnlike regression predictive modeling, time series also adds the complexity of a sequence dependence among the input variables.\n\nA powerful type of neural network designed to handle sequence dependence is called recurrent neural networks.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n#topic12345.set_index('date', inplace=True)\nplt.figure(figsize=(20, 10))\n#plt.plot(topic12345['topic1'])\n#plt.plot(topic12345['topic2'])\nplt.plot(topic12345['topic3'])\n#plt.plot(topic12345['topic4'])\n#plt.plot(topic12345['topic5'])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport matplotlib.pyplot as plt\nimport pandas\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n# fix random seed for reproducibility\nnumpy.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic3_num.set_index('date', inplace=True)\ntopic3_num = topic3_num.astype('float32')\n\n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(topic3_num)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\ntrain_size = int(len(dataset)*0.8)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ``look_back``, which is the number of previous time steps to use as input variables to predict the next time period â€” in this case defaulted to 1.\n### look_back =1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset= numpy.reshape(dataset, (dataset.shape[0], 1, dataset.shape[1]))\nyhat = model.predict(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(yhat)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n## 2. LSTM for Regression Using the Window Method\n\nRecent time steps can be used to make the prediction for the next time step. This is called a window, and the size of the window is a parameter that can be tuned for each problem.\n\nIncreasing the look_back argument from 1 to 3. When phrased as a regression problem, the input variables are t-2, t-1, t and the output variable is t+1.\n\n### look_back = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## 3. LSTM for Regression with Time Steps\n\nInstead of phrasing the past observations as separate input features, we can use them as time steps of the one input feature, which is indeed a more accurate framing of the problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(look_back, 1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. LSTM with Memory Between Batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n# create and fit the LSTM network\nbatch_size = 1\nmodel = Sequential()\nmodel.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nfor i in range(100):\n    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n    model.reset_states()\n# make predictions\ntrainPredict = model.predict(trainX, batch_size=batch_size)\nmodel.reset_states()\ntestPredict = model.predict(testX, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test set\npred_y = model.predict(des_test_x).reshape(-1,12)\n# add seasonality of the past year\nfor i in np.arange(pred_y.shape[0]-1,-1,-1):\n    pred_y[pred_y.shape[0]-1-i] += result.seasonal[-(13+i):-(1+i)]\n\nplot_evaluation(pred_y,test_y,n=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------\n# ARIMA\nARIMA stands for Auto Regressive Integrated Moving Average. While exponential smoothing models were based on a description of trend and seasonality in data, ARIMA models aim to describe the correlations in the time series. The video below explains ARIMA very well:\n\n## hyperparameters tuning:\nThe complete procedure for evaluating a grid of ARIMA hyperparameters is listed below."},{"metadata":{"trusted":true},"cell_type":"code","source":"topic12345['topic3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tune ARIMA model for topic0\n\nimport warnings\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(a, arima_order):   #a=X\n    # prepare training dataset\n    train_size = int(len(a) * 0.70)\n    train, test = a[0:train_size], a[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    error = mean_squared_error(test, predictions)\n    return error\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.3f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"topic1 Best ARIMA(2, 0, 1) MSE=716.429\n\ntopic2 Best ARIMA(0, 1, 1) MSE=4364.329\n\ntopic3 Best ARIMA(4, 1, 2) MSE=9671.969\n\ntopic4 Best ARIMA(0, 1, 1) MSE=3458.092\n\ntopic5 Best ARIMA(2, 0, 0) MSE=8961.120"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load dataset\ndef parser(x):\n    return datetime.strptime(x, '%Y-%m-%d')\nseries = topic12345['topic5']\n# evaluate parameters\np_values = [0, 1, 2, 4, 6, 8, 10]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(series.values, p_values, d_values, q_values) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"series = topic12345['topic5']\nfrom matplotlib import pyplot\nX = series.values\nsize = int(len(X) * 0.70)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(2, 0, 0))\n\tmodel_fit = model.fit(disp=0)\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\n# plot\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom statsmodels.tsa.arima_model import ARIMA\nimport numpy\n \n# create a differenced series\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn numpy.array(diff)\n \n# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n\treturn yhat + history[-interval]\n  \n# seasonal difference\nX = series.values\ndays_in_month = 10\ndifferenced = difference(X, days_in_month)\n# fit model\nmodel = ARIMA(differenced, order=(2, 0, 0) )\nmodel_fit = model.fit(disp=0)\n# multi-step out-of-sample forecast\nstart_index = len(differenced)\nend_index = start_index + 30\nforecast = model_fit.predict(start=start_index, end=end_index)\n# invert the differenced forecast to something usable\nhistory = [x for x in X]\nday = 1\n\nfor yhat in forecast:\n\tinverted = inverse_difference(history, yhat, days_in_month)\n\tprint('Day %d: %f' % (day, inverted))\n\thistory.append(inverted)\n\tday += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an array for prediction \nrng = pd.date_range('2020-04-18', periods=31, freq='D')\nextenddf = pd.DataFrame({ 'date': rng, 'topic5' : history[106:len(history)]}) \nextenddf.set_index('date', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction31 = pd.merge(prediction31,\n                       extenddf,\n                       how = 'outer',\n                       on = 'date')\n\n#prediction31 = prediction31.drop(['anger_y','love_y','enthusiasm_y'],axis = 1)\n#prediction31.rename(columns={'anger_x': 'anger','love_x':'love','enthusiasm_x':'enthusiasm'}, inplace=True) \n#prediction31 = prediction31.abs()\nprediction31","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction31.to_csv('predtopics913.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nprediction31 = DataFrame(extenddf,columns=['topic3'])\nprediction31.index= extenddf.index.astype('datetime64[ns]')\nprediction31","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----\n## previous work "},{"metadata":{"trusted":true},"cell_type":"code","source":"y1 = topic4_num['topic4_num']\ny2 = topic4_num['topic4_num'].resample('MS').mean()\ny1.plot(figsize=(15, 6))\ny2.plot(figsize=(15, 6))\nplt.ylabel('Number of topic 4 documents')\nplt.xlabel('Timeline ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y3 = topic3_num['topic3_num']\ny4 = topic3_num['topic3_num'].resample('MS').mean()\ny3.plot(figsize=(15, 6))\ny4.plot(figsize=(15, 6))\nplt.ylabel('Number of topic 3 documents')\nplt.xlabel('Timeline ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y5 = topic2_num['topic2_num']\ny6 = topic2_num['topic2_num'].resample('MS').mean()\ny5.plot(figsize=(15, 6))\ny6.plot(figsize=(15, 6))\nplt.ylabel('Number of topic 2 documents')\nplt.xlabel('Timeline ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y7 = topic1_num['topic1_num']\ny8 = topic1_num['topic1_num'].resample('MS').mean()\ny7.plot(figsize=(15, 6))\ny8.plot(figsize=(15, 6))\nplt.ylabel('Number of topic 1 documents')\nplt.xlabel('Timeline ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y9 = topic0_num['topic0_num']\ny10 = topic0_num['topic0_num'].resample('MS').mean()\ny9.plot(figsize=(15, 6))\ny10.plot(figsize=(15, 6))\nplt.ylabel('Number of topic 0 documents')\nplt.xlabel('Timeline ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic0_num = topic0_num.set_index('date')\ny0 = topic0_num['topic0_num']\n\ntopic1_num = topic1_num.set_index('date')\ny1 = topic1_num['topic1_num']\n\ntopic2_num = topic2_num.set_index('date')\ny2 = topic2_num['topic2_num']\n\ntopic3_num = topic3_num.set_index('date')\ny3 = topic3_num['topic3_num']\n\ntopic4_num = topic4_num.set_index('date')\ny4 = topic4_num['topic4_num']\n\ny0.plot(figsize=(15, 6))\ny1.plot(figsize=(15, 6))\ny2.plot(figsize=(15, 6))\ny3.plot(figsize=(15, 6))\ny4.plot(figsize=(15, 6))\nplt.ylabel('Number of Reddit Comments')\nplt.xlabel('Timeline ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df01 = pd.merge(topic0_num,\n                    topic1_num,\n                    how = 'outer',\n                    on = 'date')\ndf012 = pd.merge(df01,\n                    topic2_num,\n                    how = 'outer',\n                    on = 'date')\ndf0123 = pd.merge(df012,\n                    topic3_num,\n                    how = 'outer',\n                    on = 'date')\ndf01234 = pd.merge(df0123,\n                    topic4_num,\n                    how = 'outer',\n                    on = 'date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df01234['topic0_num'] = df01234['topic0_num'].fillna(0.0)\ndf01234['topic1_num'] = df01234['topic1_num'].fillna(0.0)\ndf01234['topic2_num'] = df01234['topic2_num'].fillna(0.0)\ndf01234['topic3_num'] = df01234['topic3_num'].fillna(0.0)\ndf01234['topic4_num'] = df01234['topic4_num'].fillna(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df01234.to_csv('topics_numbers')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## train/Val split \n* moving average "},{"metadata":{"trusted":true},"cell_type":"code","source":"# topic0\ntrain_dataset = df01234[-106:-30]\nval_dataset = df01234[-30:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = []\nfor i in range(len(val_dataset.columns)):\n    if i == 0:\n        predictions.append(np.mean(train_dataset[train_dataset.columns[-30:]].values, axis=1))\n    if i < 31 and i > 0:\n        predictions.append(0.5 * (np.mean(train_dataset[train_dataset.columns[-30+i:]].values, axis=1) + \\\n                                  np.mean(predictions[:i], axis=0)))\n    if i > 31:\n        predictions.append(np.mean([predictions[:i]], axis=1))\n    \npredictions = np.transpose(np.array([row.tolist() for row in predictions]))\nerror_avg = np.linalg.norm(predictions[:3] - val_dataset.values[:3])/len(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\npred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"Moving average\")\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}