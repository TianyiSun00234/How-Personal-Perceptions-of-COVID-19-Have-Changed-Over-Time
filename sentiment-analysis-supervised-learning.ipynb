{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nfrom numpy import random\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport re\nimport logging        \nimport gensim\nimport xgboost as xgb\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom bs4 import BeautifulSoup\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\ndf = pd.read_csv('/kaggle/input/train-cleaned/trainset_cleaned.csv')\ntrain = df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------\n* Using **oversampling** to avoid information lose!\n* first train test split, second oversampling!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ntrain.groupby('sentiment').clean_content.count().plot.bar(ylim=0)\n#plt.show()\n\n## label data \n#from sklearn import preprocessing\n#le = preprocessing.LabelEncoder()\n#train['sentiment_L'] = le.fit_transform(train['sentiment'])\n#train.head(3)\n\n# train-test split \nX = train[['clean_content','sentiment']]\ny = train.sentiment\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\n\n\n# oversampling on training set\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler()\n#X_ros,y_ros  = ros.fit_sample(X, y)\nX_ros,y_ros  = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X_train.shape[0], 'new random picked points')\n\nX_ros.index.name = 'id'\n#y_ros.index.name = 'id'\n#train_ros = pd.merge(X_ros,\n#                    y_ros,\n#                    how = 'left',\n#                    on = 'id')\nprint('Train set shape after oversampling: ', X_ros.shape )\n\n# plot balanced\nfig = plt.figure(figsize=(8,6))\nX_ros.groupby('sentiment').clean_content.count().plot.bar(ylim=0)\nplt.show()\n\n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\n\ntest = pd.read_csv('/kaggle/input/topic907/topics.csv')\n\n##Logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(X_ros.clean_content.values.astype('U'), X_ros.sentiment.values.astype('U'))\n\ny_pred = logreg.predict(test['clean_content'][100000:200000].values.astype('U'))\nprint('Logistic regression')\n#print('accuracy %s' % accuracy_score(y_pred, y_test))\n#print(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(test['clean_content'][:40000].values.astype('U'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test['clean_content'][:40000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\ny_pred = pd.DataFrame(y_pred, columns = ['sentiment'] )\ny_pred.to_csv('sapred40000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"------"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ntrain.groupby('sentiment').clean_content.count().plot.bar(ylim=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train-test split \nX = train[['clean_content','sentiment']]\ny = train.sentiment\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\n# oversampling on training set\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler()\nX_ros,y_ros  = ros.fit_sample(X, y)\n\nprint(X_ros.shape[0] - X_train.shape[0], 'new random picked points')\nX_ros.index.name = 'id'\nprint('Train set shape after oversampling: ', X_ros.shape )\n\n# plot balanced\nfig = plt.figure(figsize=(8,6))\nX_ros.groupby('sentiment').clean_content.count().plot.bar(ylim=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## supervised learning\n\n* step1: feature extraction [sklearn.feature_extraction.text ](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)\nThe goal of using **tf-idf** instead of the raw frequencies of occurrence of a token in a given document is to **scale down** the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. "},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\n\nX_train = X_ros.clean_content\ny_train = X_ros.sentiment\nX_test = X_test.clean_content\nprint(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n##Naive Bayes classifier\nfrom sklearn.naive_bayes import MultinomialNB\nnb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB())])\nnb.fit(X_train.values.astype('U'), y_train)\ny_pred = nb.predict(X_test.values.astype('U'))\nprint('Naive Bayes classifier')\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))\n\n##Linear SVM\nfrom sklearn.linear_model import SGDClassifier\nsgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n               ])\nsgd.fit(X_train.values.astype('U'), y_train)\n\ny_pred = sgd.predict(X_test.values.astype('U'))\nprint('Liear SVM')\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))\n\n##Logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(X_train.values.astype('U'), y_train)\n\ny_pred = logreg.predict(X_test.values.astype('U'))\nprint('Logistic regression')\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))\n\n## LinearSVC\nfrom sklearn.svm import LinearSVC\nlsvc = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LinearSVC()),\n               ])\nlsvc.fit(X_train.values.astype('U'), y_train)\ny_pred = lsvc.predict(X_test.values.astype('U'))\nprint('LinearSVC')\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))\n\n## RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nrf = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', RandomForestClassifier(max_depth=2, random_state = 42)),\n               ])\nrf.fit(X_train.values.astype('U'), y_train)\ny_pred = lsvc.predict(X_test.values.astype('U'))\nprint('RandomForestClassifier')\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/topic907/topics.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Logistic regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(X_ros.clean_content.values.astype('U'), X_ros.sentiment.values.astype('U'))\n\ny_pred = logreg.predict(test['clean_content'][0:40000].values.astype('U'))\nprint('Logistic regression')\n#print('accuracy %s' % accuracy_score(y_pred, y_test))\n#print(classification_report(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_content'][40000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/topic907/topics.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(test['clean_content'][40000:].values.astype('U'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.to_csv('sapred40000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logreg.predict(test.clean_content)\ntest['sentiment'] = y_pred\ntest.to_csv('sentiment_test_results.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nmodels = [nb, sgd, logreg, lsvc, rf]\n\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, X_train, y_train, cv=CV, scoring='accuracy')\n    #accuracies = accuracy_score(y_pred, y_test)\n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n    \nimport seaborn as sns\n\nsns.boxplot(x='fold_idx', y='accuracy', data=cv_df)\nsns.stripplot(x='fold_idx', y='accuracy', data=cv_df, \n              size=16, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()\n\ncv_df.groupby('fold_idx').accuracy.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['category_id'] = train['sentiment'].factorize()[0]\nfrom io import StringIO\ncategory_id_df = train[['sentiment', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'sentiment']].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(train.clean_content).toarray()\nlabels = train.category_id\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels = category_id_df.sentiment.values, yticklabels=category_id_df.sentiment.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since feature and feature1's shape are the same so there is no duplications so we we can use the same train_text_split as before. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.feature_selection import chi2\n#import numpy as np\n\n#N = 2\n#for Product, category_id in sorted(category_to_id.items()):\n#    features_chi2 = chi2(features, labels == category_id)\n#    indices = np.argsort(features_chi2[0])\n#    feature_names = np.array(tfidf.get_feature_names())[indices]\n#    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n#    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n#    print(\"# '{}':\".format(Product))\n#    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n#    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import display\n\nfor predicted in category_id_df.category_id:\n    for actual in category_id_df.category_id:\n        if predicted != actual and conf_mat[actual, predicted] >= 6:\n            print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n            display(train2.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['sentiment', 'text']])\n            print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(features, labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import chi2\n\nN = 2\nfor Product, category_id in sorted(category_to_id.items()):\n    indices = np.argsort(model.coef_[category_id])\n    feature_names = np.array(tfidf.get_feature_names())[indices]\n    unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n    bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n    print(\"# '{}':\".format(Product))\n    print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n    print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, \n                                    target_names=train2['sentiment'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------\nWord2vec embedding and Logistic Regression\n\nDoc2vec and Logistic Regression\n\n-- Taking the linear combination of every term in the document creates a random walk with bias process in the word2vec space."},{"metadata":{},"cell_type":"markdown","source":"------------\n# LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install tweet-preprocessor 2>/dev/null 1>/dev/null\n\n!pip install transformers\n!pip install tensorboardx\n!pip install simpletransformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom bs4 import BeautifulSoup\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\n\nimport re\n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\n\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nfrom IPython.core.interactiveshell import InteractiveShell\n\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.offline import iplot\nimport tokenizer\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\n\nSTOPWORDS = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## label data \n#from sklearn import preprocessing\n#le = preprocessing.LabelEncoder()\n#train['sentiment_L'] = le.fit_transform(train['sentiment'])\n#train.head(3)\n\n# train-test split \nX = train[['clean_content','sentiment']]\ny = train.sentiment\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n\n\n\n# oversampling on training set\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler()\n#X_ros,y_ros  = ros.fit_sample(X, y)\nX_ros,y_ros  = ros.fit_sample(X, y)\n\n#print(X_ros.shape[0] - X_train.shape[0], 'new random picked points')\n\nX_ros.index.name = 'id'\n#y_ros.index.name = 'id'\n#train_ros = pd.merge(X_ros,\n#                    y_ros,\n#                    how = 'left',\n#                    on = 'id')\nprint('Train set shape after oversampling: ', X_ros.shape )\n\n# plot balanced\nfig = plt.figure(figsize=(8,6))\nX_ros.groupby('sentiment').clean_content.count().plot.bar(ylim=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize what was learned\nprint(t.word_counts)\nprint(t.document_count)\nprint(t.word_index)\n#print(t.word_docs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n# create the tokenizer\nt = Tokenizer()\n# fit the tokenizer on the documents\nt.fit_on_texts(X_ros['clean_content'].values.astype(str))\n\n#X_ros = df[['clean_content','sentiment']]\n# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 250\n# This is fixed.\nEMBEDDING_DIM = 100\n\n\nX = t.texts_to_sequences(X_ros['clean_content'].values.astype(str))\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\n\nY = pd.get_dummies(X_ros['sentiment']).values\nprint('Shape of label tensor:', Y.shape)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n\nmodel = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(13, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n\nepochs = 8\nbatch_size = 64\n\nhistory = model.fit(X, Y, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n\naccr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))\n\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show();\n\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(X_ros['sentiment']).values\nprint('Shape of label tensor:', Y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model to single file\nmodel.save('lstm_model.h1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_pred = t.texts_to_sequences(test['clean_content'].values.astype(str))\n#X_pred = pad_sequences(X_pred, maxlen=MAX_SEQUENCE_LENGTH)\n#y_predict = model.predict_classes(X_pred)\n#test['sentiments'] = y_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/topic907/topics.csv')\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_content'][:10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = t.texts_to_sequences(test['clean_content'][:10000].values.astype(str))\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nyhat10000 = model.predict_classes(X_test)\nfrom pandas import DataFrame\nyhat10000 = pd.DataFrame(yhat10000, columns = ['sentiment'] )\nyhat10000.to_csv('sapred_10000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_content'][10000:20000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = t.texts_to_sequences(test['clean_content'][10000:20000].values.astype(str))\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nyhat20000 = model.predict_classes(X_test)\nfrom pandas import DataFrame\nyhat20000 = pd.DataFrame(yhat20000, columns = ['sentiment'] )\nyhat20000.to_csv('sapred_20000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_content'][20000:30000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = t.texts_to_sequences(test['clean_content'][20000:30000].values.astype(str))\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nyhat30000 = model.predict_classes(X_test)\nyhat30000 = pd.DataFrame(yhat30000, columns = ['sentiment'] )\nyhat30000.to_csv('sapred_30000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_content'][30000:40000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = t.texts_to_sequences(test['clean_content'][30000:40000].values.astype(str))\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nyhat40000 = model.predict_classes(X_test)\nfrom pandas import DataFrame\nyhat40000 = pd.DataFrame(yhat40000, columns = ['sentiment'] )\nyhat40000.to_csv('sapred_40000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/topic907/topics.csv')\ntest\ntest['clean_content'][40000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = t.texts_to_sequences(test['clean_content'][40000:].values.astype(str))\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nyhat50000 = model.predict_classes(X_test)\nyhat50000 = pd.DataFrame(yhat50000, columns = ['sentiment'] )\nyhat50000.to_csv('sapred_50000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nMAX_SEQUENCE_LENGTH = 250\nfrom keras.preprocessing.text import Tokenizer\n# create the tokenizer\nt = Tokenizer()\nfrom keras.models import load_model\n# load model from single file\nmodel = load_model('lstm_model.h1')\n# make predictions\nX_test = t.texts_to_sequences(test['clean_content'][10000:20000].values.astype(str))\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nyhat = model.predict_classes(X_test)\n#target['sent_LSTM'] = yhat\n#target.sentiment = le.inverse_transform(target.sentlabel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat.to_cav('sa10000.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making predictions using previous models\nylr = logreg.predict(target.samples)\nyLsvm = sgd.predict(target.samples)\n ynb = nb.predict(target.samples)\nylr = logreg.predict(target.samples)\nyLsvm = sgd.predict(target.samples)\nynb = nb.predict(target.samples)\nylr = logreg.predict(target.samples)\ntarget['sent_logreg'] = ylr\ntarget['sent_Lsvm'] = yLsvm\ntarget['sent_Naive_Bayes'] = ynb\ntarget.rename(columns={\"sentiment\": \"sent_lstm\"})\ntarget = target.drop(['confident_score','sentlabel'], axis=1)\ntarget.loc['accuracy'] = ['', '0.880', '0.8836597005101201','0.7806483462234656', '0.8025341451374033']\nyLsvc = lsvc.predict(target.samples)\ntarget['sent_logreg'] = yLsvc\ntarget.to_csv('sample_results.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import types\nimport tempfile\nimport keras.models\n\ndef make_keras_picklable():\n    def __getstate__(self):\n        model_str = \"\"\n        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n            keras.models.save_model(self, fd.name, overwrite=True)\n            model_str = fd.read()\n        d = { 'model_str': model_str }\n        return d\n\n    def __setstate__(self, state):\n        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n            fd.write(state['model_str'])\n            fd.flush()\n            model = keras.models.load_model(fd.name)\n        self.__dict__ = model.__dict__\n\n\n    cls = keras.models.Model\n    cls.__getstate__ = __getstate__\n    cls.__setstate__ = __setstate__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport pickle\n\nmake_keras_picklable()\n\nm = keras.models.Sequential()\nm.add(keras.layers.Dense(10, input_shape=(10,)))\nm.compile(optimizer='sgd', loss='mse')\n\npickle.dumps(m)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load model \nmake_keras_picklable()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model.evaluate(X_test,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}