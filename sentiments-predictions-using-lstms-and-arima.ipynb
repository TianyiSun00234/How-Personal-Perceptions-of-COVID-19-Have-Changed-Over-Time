{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df4 = pd.read_csv('/kaggle/input/testset/april_comments.csv')\ndf3 = pd.read_csv('/kaggle/input/testset/march_comments.csv')\ndf1 = pd.read_csv('/kaggle/input/testset/january_comments.csv')\ndf2 = pd.read_csv('/kaggle/input/testset/february_comments.csv')\ntopics = pd.read_csv('/kaggle/input/topic907/topics.csv')\nsentiments = pd.read_csv('/kaggle/input/topic907/sentiment912.csv')\n\nframe = pd.concat([df1, df2, df3, df4], ignore_index=True)\nframe[['date','author','preview']]\n\nresult1 = pd.merge(frame[['date','author','preview']],\n                   topics[['clean_content','token1','topic1']],\n                   how = 'inner',\n                   left_index=True, right_index=True)\n\nresult = pd.merge(result1,\n                 sentiments,\n                 how = 'inner',\n                   left_index=True, right_index=True)\nresult.rename(columns={\"token1\": \"tokens\", \"topic1\": \"topics\"}, inplace=True)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# sentiments"},{"metadata":{"trusted":true},"cell_type":"code","source":"result.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"anxiety       219149\nrelaxation     66288\nsadness        56941\nneutral        39894\nfear           13916\nanger           8934\nlove            1036\ndesire          1024\nfun              823\nenthusiasm       780\nhappiness        437\ndisgust          222\nboredom           32 "},{"metadata":{"trusted":true},"cell_type":"code","source":"anxiety = df.loc[df['sentiment'] == 'anxiety']\nanxiety_num = anxiety.groupby('date')['sentiment'].count().reset_index()\nanxiety_num.rename(columns ={'sentiment': 'anxiety'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relaxation = df.loc[df['sentiment'] == 'relaxation']\nrelaxation_num = relaxation.groupby('date')['sentiment'].count().reset_index()\nrelaxation_num.rename(columns ={'sentiment': 'relaxation'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sadness = df.loc[df['sentiment'] == 'sadness']\nsadness_num = sadness.groupby('date')['sentiment'].count().reset_index()\nsadness_num.rename(columns ={'sentiment': 'sadness'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral = df.loc[df['sentiment'] == 'neutral']\nneutral_num = neutral.groupby('date')['sentiment'].count().reset_index()\nneutral_num.rename(columns ={'sentiment': 'neutral'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fear = df.loc[df['sentiment'] == 'fear']\nfear_num = fear.groupby('date')['sentiment'].count().reset_index()\nfear_num.rename(columns ={'sentiment': 'fear'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anger = df.loc[df['sentiment'] == 'anger']\nanger_num = anger.groupby('date')['sentiment'].count().reset_index()\nanger_num.rename(columns ={'sentiment': 'anger'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"love = df.loc[df['sentiment'] == 'love']\nlove_num = love.groupby('date')['sentiment'].count().reset_index()\nlove_num.rename(columns ={'sentiment': 'love'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"desire = df.loc[df['sentiment'] == 'desire']\ndesire_num = desire.groupby('date')['sentiment'].count().reset_index()\ndesire_num.rename(columns ={'sentiment': 'desire'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fun = df.loc[df['sentiment'] == 'fun']\nfun_num = fun.groupby('date')['sentiment'].count().reset_index()\nfun_num.rename(columns ={'sentiment': 'fun'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enthusiasm = df.loc[df['sentiment'] == 'enthusiasm']\nenthusiasm_num = enthusiasm.groupby('date')['sentiment'].count().reset_index()\nenthusiasm_num.rename(columns ={'sentiment': 'enthusiasm'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"happiness = df.loc[df['sentiment'] == 'happiness']\nhappiness_num = happiness.groupby('date')['sentiment'].count().reset_index()\nhappiness_num.rename(columns ={'sentiment': 'happiness'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disgust = df.loc[df['sentiment'] == 'disgust']\ndisgust_num = disgust.groupby('date')['sentiment'].count().reset_index()\ndisgust_num.rename(columns ={'sentiment': 'disgust'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boredom = df.loc[df['sentiment'] == 'boredom']\nboredom_num = boredom.groupby('date')['sentiment'].count().reset_index()\nboredom_num.rename(columns ={'sentiment': 'boredom'}, inplace=True )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsentiment = pd.merge(anxiety_num,\n                relaxation_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                sadness_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                neutral_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                fear_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                anger_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                love_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                desire_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                fun_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                enthusiasm_num,\n                how = 'outer',\n                on = 'date')\n\n\nsentiment = pd.merge(sentiment,\n                happiness_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                disgust_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment = pd.merge(sentiment,\n                boredom_num,\n                how = 'outer',\n                on = 'date')\n\nsentiment['anxiety'] = sentiment['anxiety'].fillna(0.0)\nsentiment['relaxation'] = sentiment['relaxation'].fillna(0.0)\nsentiment['sadness'] = sentiment['sadness'].fillna(0.0)\nsentiment['neutral'] = sentiment['neutral'].fillna(0.0)\nsentiment['fear'] = sentiment['fear'].fillna(0.0)\nsentiment['anger'] = sentiment['anger'].fillna(0.0)\nsentiment['love'] = sentiment['love'].fillna(0.0)\nsentiment['desire'] = sentiment['desire'].fillna(0.0)\nsentiment['fun'] = sentiment['fun'].fillna(0.0)\nsentiment['enthusiasm'] = sentiment['enthusiasm'].fillna(0.0)\nsentiment['happiness'] = sentiment['happiness'].fillna(0.0)\nsentiment['disgust'] = sentiment['disgust'].fillna(0.0)\nsentiment['boredom'] = sentiment['boredom'].fillna(0.0)\nsentiment = sentiment.sort_values(by='date',ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment.to_csv('countsentiments912.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----\n# [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nsentiment.set_index('date', inplace=True)\nplt.figure(figsize=(20, 10))\nplt.plot(sentiment['anxiety'])\nplt.plot(sentiment['relaxation'])\nplt.plot(sentiment['sadness'])\nplt.plot(sentiment['neutral'])\nplt.plot(sentiment['fear'])\n\nplt.plot(sentiment['anger'])\nplt.plot(sentiment['love'])\nplt.plot(sentiment['desire'])\nplt.plot(sentiment['fun'])\nplt.plot(sentiment['enthusiasm'])\n\nplt.plot(sentiment['happiness'])\nplt.plot(sentiment['disgust'])\nplt.plot(sentiment['boredom'])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"anxiety       219149\nrelaxation     66288\nsadness        56941\nneutral        39894\nfear           13916\nanger           8934\nlove            1036\ndesire          1024\nfun              823\nenthusiasm       780\nhappiness        437\ndisgust          222\nboredom           32\nName: sentiment, dtype: int64"},{"metadata":{"trusted":true},"cell_type":"code","source":"anxiety_num.set_index('date', inplace=True)\nplt.plot(anxiety_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM Network for Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\nimport matplotlib.pyplot as plt\nimport pandas\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fix random seed for reproducibility\nnumpy.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  extract the NumPy array from the dataframe \n#and convert the integer values to floating point values\n#, which are more suitable for modeling with a neural network.\nanxiety_num = anxiety_num.astype('float32')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(anxiety_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\ntrain_size = int(len(dataset)*0.8)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The ``look_back``, which is the number of previous time steps to use as input variables to predict the next time period — in this case defaulted to 1.\n### look_back =1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset= numpy.reshape(dataset, (dataset.shape[0], 1, dataset.shape[1]))\nyhat = model.predict(dataset)\nplt.plot(yhat)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n## 2. LSTM for Regression Using the Window Method\n\nRecent time steps can be used to make the prediction for the next time step. This is called a window, and the size of the window is a parameter that can be tuned for each problem.\n\nIncreasing the look_back argument from 1 to 3. When phrased as a regression problem, the input variables are t-2, t-1, t and the output variable is t+1.\n\n### look_back = 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\n## 3. LSTM for Regression with Time Steps\n\nInstead of phrasing the past observations as separate input features, we can use them as time steps of the one input feature, which is indeed a more accurate framing of the problem."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(look_back, 1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. LSTM with Memory Between Batches"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape into X=t and Y=t+1\nlook_back = 3\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n# create and fit the LSTM network\nbatch_size = 1\nmodel = Sequential()\nmodel.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nfor i in range(100):\n    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n    model.reset_states()\n# make predictions\ntrainPredict = model.predict(trainX, batch_size=batch_size)\nmodel.reset_states()\ntestPredict = model.predict(testX, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict test set\npred_y = model.predict(des_test_x).reshape(-1,12)\n# add seasonality of the past year\nfor i in np.arange(pred_y.shape[0]-1,-1,-1):\n    pred_y[pred_y.shape[0]-1-i] += result.seasonal[-(13+i):-(1+i)]\n\nplot_evaluation(pred_y,test_y,n=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-----"},{"metadata":{"trusted":true},"cell_type":"code","source":"relaxation_num.set_index('date', inplace=True)\nplt.plot(relaxation_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sadness_num.set_index('date', inplace=True)\nplt.plot(sadness_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neutral_num.set_index('date', inplace=True)\nplt.plot(neutral_num)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-------\n# ARIMA\nARIMA stands for Auto Regressive Integrated Moving Average. While exponential smoothing models were based on a description of trend and seasonality in data, ARIMA models aim to describe the correlations in the time series. The video below explains ARIMA very well:\n\n## hyperparameters tuning:\nThe complete procedure for evaluating a grid of ARIMA hyperparameters is listed below."},{"metadata":{"trusted":true},"cell_type":"code","source":"anxiety_num['anxiety']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tune ARIMA model for topic0\n\nimport warnings\nfrom pandas import read_csv\nfrom pandas import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(a, arima_order):   #a=X\n    # prepare training dataset\n    train_size = int(len(a) * 0.70)\n    train, test = a[0:train_size], a[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    error = mean_squared_error(test, predictions)\n    return error\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(dataset, order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.3f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anxiety       219149\nrelaxation     66288\nsadness        56941\nneutral        39894\nfear           13916\nanger           8934\nlove            1036\ndesire          1024\nfun              823\nenthusiasm       780\nhappiness        437\ndisgust          222\nboredom           32 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## make prediction "},{"metadata":{},"cell_type":"markdown","source":"anxiety: Best ARIMA(1, 0, 0) MSE=8070.985  89.839\n\nrelaxation Best ARIMA(0, 2, 2) MSE=2734.418 52.292\n\nsadness Best ARIMA(0, 2, 2) MSE=2999.558 54.768\n\nneutral Best ARIMA(0, 1, 1) MSE=2605.227 51.042\n\nfear Best ARIMA(0, 1, 1) MSE=3151.195 56.136\n\nanger Best ARIMA(6, 0, 0) MSE=453.518 21.300\n\nlove Best ARIMA(0, 1, 2) MSE=35.801 5.983\n\nfun Best ARIMA(4, 1, 0) MSE=39.101 6.253\n\ndesire Best ARIMA(2, 0, 0) MSE=11.942 3.456\n\nenthusiasm Best ARIMA(0, 1, 1) MSE=29.535 5.435\n\nhappiness Best ARIMA(2, 0, 0) MSE=10.448 3.232\n\ndisgust Best ARIMA(2, 0, 0) MSE=2.195 1.4816\n\nboredom Best ARIMA(0, 1, 1) MSE=0.488 0.699"},{"metadata":{"trusted":true},"cell_type":"code","source":"series = boredom_num['boredom']\nfrom matplotlib import pyplot\nX = series.values\nsize = int(len(X) * 0.70)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n\tmodel = ARIMA(history, order=(0,1,1))\n\tmodel_fit = model.fit(disp=0)\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test[t]\n\thistory.append(obs)\n\tprint('predicted=%f, expected=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\n# plot\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\nfrom statsmodels.tsa.arima_model import ARIMA\nimport numpy\n \n# create a differenced series\ndef difference(dataset, interval=1):\n\tdiff = list()\n\tfor i in range(interval, len(dataset)):\n\t\tvalue = dataset[i] - dataset[i - interval]\n\t\tdiff.append(value)\n\treturn numpy.array(diff)\n \n# invert differenced value\ndef inverse_difference(history, yhat, interval=1):\n\treturn yhat + history[-interval]\n  \n# seasonal difference\nX = series.values\ndays_in_month = 10\ndifferenced = difference(X, days_in_month)\n# fit model\nmodel = ARIMA(differenced, order=(0,1,1))\nmodel_fit = model.fit(disp=0)\n# multi-step out-of-sample forecast\nstart_index = len(differenced)\nend_index = start_index + 30\nforecast = model_fit.predict(start=start_index, end=end_index)\n# invert the differenced forecast to something usable\nhistory = [x for x in X]\nday = 1\n\nfor yhat in forecast:\n\tinverted = inverse_difference(history, yhat, days_in_month)\n\tprint('Day %d: %f' % (day, inverted))\n\thistory.append(inverted)\n\tday += 1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an array for prediction \nrng = pd.date_range('2020-04-18', periods=31, freq='D')\nextenddf = pd.DataFrame({ 'date': rng, 'boredom' : history[17:len(history)]}) \nextenddf.set_index('date', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nprediction31 = DataFrame(extenddf,columns=['anxiety'])\nprediction31.index= extenddf.index.astype('datetime64[ns]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction31 = pd.merge(prediction31,\n                       extenddf,\n                       how = 'outer',\n                       on = 'date')\n\nprediction31 = prediction31.drop(['anger_y','love_y','enthusiasm_y'],axis = 1)\nprediction31.rename(columns={'anger_x': 'anger','love_x':'love','enthusiasm_x':'enthusiasm'}, inplace=True) \nprediction31 = prediction31.abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction31.to_csv('predsent913.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}