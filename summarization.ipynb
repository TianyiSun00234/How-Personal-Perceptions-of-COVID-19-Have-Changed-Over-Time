{"cells":[{"metadata":{},"cell_type":"markdown","source":"dataset reference:biorxiv_clean.csv from [COVID-19 Open Research Dataset Challenge (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) \n\ntechnical reference: [Summarization of COVID research papers using BART model](https://towardsdatascience.com/summarization-of-covid-research-papers-using-bart-model-5b109a6669a6)\n\nSummarization based on output text: either abstractive or extractive. \n* abstractive: understanding and rewrite it. \n* extractive: select a subset of text and highlighting it. \n\nknow more about summarization: [Unsupervised Text Summarization using Sentence Embeddings](https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install langdetect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install talon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install skipthoughts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r email-summarization/requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk; nltk.download(\"punkt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import talon\nfrom talon.signature.bruteforce import extract_signature\nfrom langdetect import detect\nfrom nltk.tokenize import sent_tokenize\nimport skipthoughts\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r email-summarization/requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import email_summarization\nfrom email_summarization import summarize\nsummaries = summarize(emails)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction = pd.read_csv('/kaggle/input/contractionwords/contraction words.csv')\ncontraction_ = pd.read_csv('/kaggle/input/contractions/contractions.csv')\ndf = pd.read_csv('/kaggle/input/dataworld1/query_result.csv')\ncontraction = contraction.T\ncontraction = contraction.rename_axis('Contraction')\ncontraction.columns =['Meaning'] \ncontraction = contraction.reset_index()\ncontractions = pd.merge(contraction,contraction_, on ='Contraction',how = 'outer' )\n'''did not finish so just use the previous one '''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import pipeline\nsummarizer = pipeline('summarization')\nfor i, text in enumerate(data):\n    print(summarizer(data['text'].iloc[i], max_length=1000,   min_length=30))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## step1: preprocessing  "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tweet-preprocessor 2>/dev/null 1>/dev/null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import preprocessor as p\nimport emoji\np.set_options(p.OPT.MENTION, p.OPT.URL)\n#p.clean(\"hello guys @alx #sportðŸ”¥ 1245 https://github.com/s/preprocessor\")\n\ndef punctuation(val): \n    punctuations = '''@[A-)?!(0-9_]+-[]{};:'\"\\,<>. `./@#$%^&_~*'''\n    for x in val:                                                                  \n        if x in punctuations: \n            val = val.replace(x, \" \") \n    return val\n\n\ncontractions = pd.read_csv(\"/kaggle/input/contractions/contractions.csv\")\ncontractions['Contraction'] = contractions.Contraction.apply(lambda x : x.lower())\ncontractions['Meaning'] = contractions.Meaning.apply(lambda x : x.lower())\ncont_dic = dict(zip(contractions.Contraction, contractions.Meaning))\ndef cont_to_meaning(val): \n  \n    for x in val.split(): \n        if x in cont_dic.keys(): \n            val = val.replace(x, cont_dic[x]) \n    return val\n\n\n\nmisspell_data = pd.read_csv(\"/kaggle/input/spelling/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\nmisspell_data.misspell = misspell_data.misspell.str.strip()\nmisspell_data.misspell = misspell_data.misspell.str.split(\" \")\nmisspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\nmisspell_data.drop_duplicates(\"misspell\",inplace=True)\nmiss_corr = dict(zip(misspell_data.misspell.apply(lambda x : x.lower()), misspell_data.correction.apply(lambda x : x.lower())))\ndef misspelled_correction(val):\n    for x in val.split():      \n        if x in miss_corr.keys(): \n            val = val.replace(x, miss_corr[x]) \n    return val\n\n\ndef clean_text(val):    \n    b = p.clean(val).lower()\n    c = cont_to_meaning(misspelled_correction(emoji.demojize(b)))\n    d = punctuation(c)    \n    return d\n    \n\ntrain['clean_content'] = train.content.apply(lambda x : clean_text(x))\ntrain.head(2)\n\n\ndf['clean_content'] =df.content.apply(lambda x : clean_text(x))\n\ndf = df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(emails):\n    \"\"\"\n    Performs preprocessing operations such as:\n        1. Removing signature lines (only English emails are supported)\n        2. Removing new line characters.\n    \"\"\"\n    n_emails = len(emails)\n    for i in range(n_emails):\n        email = emails[i]\n        email, _ = extract_signature(email)\n        lines = email[0].split('\\n')\n        for j in reversed(range(len(lines))):\n            lines[j] = lines[j].strip()\n            if lines[j] == '':\n                lines.pop(j)\n        emails[i] = ' '.join(lines)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess (df.clean_content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef split_sentences(emails):\n    \"\"\"\n    Splits the emails into individual sentences\n    \"\"\"\n    n_emails = len(emails)\n    for i in range(n_emails):\n        email = emails[i]\n        sentences = sent_tokenize(email)\n        for j in reversed(range(len(sentences))):\n            sent = sentences[j]\n            sentences[j] = sent.strip()\n            if sent == '':\n                sentences.pop(j)\n        emails[i] = sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def skipthought_encode(emails):\n    \"\"\"\n    Obtains sentence embeddings for each sentence in the emails\n    \"\"\"\n    enc_emails = [None]*len(emails)\n    cum_sum_sentences = [0]\n    sent_count = 0\n    for email in emails:\n        sent_count += len(email)\n        cum_sum_sentences.append(sent_count)\n\n    all_sentences = [sent for email in emails for sent in email]\n    print('Loading pre-trained models...')\n    model = skipthoughts.load_model()\n    encoder = skipthoughts.Encoder(model)\n    print('Encoding sentences...')\n    enc_sentences = encoder.encode(all_sentences, verbose=False)\n\n    for i in range(len(emails)):\n        begin = cum_sum_sentences[i]\n        end = cum_sum_sentences[i+1]\n        enc_emails[i] = enc_sentences[begin:end]\n    return enc_emails\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def summarize(emails):\n    \"\"\"\n    Performs summarization of emails\n    \"\"\"\n    n_emails = len(emails)\n    summary = [None]*n_emails\n    print('Preprecesing...')\n    preprocess(emails)\n    print('Splitting into sentences...')\n    split_sentences(emails)\n    print('Starting to encode...')\n    enc_emails = skipthought_encode(emails)\n    print('Encoding Finished')\n    for i in range(n_emails):\n        enc_email = enc_emails[i]\n        n_clusters = int(np.ceil(len(enc_email)**0.5))\n        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n        kmeans = kmeans.fit(enc_email)\n        avg = []\n        closest = []\n        for j in range(n_clusters):\n            idx = np.where(kmeans.labels_ == j)[0]\n            avg.append(np.mean(idx))\n        closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_,\\\n                                                   enc_email)\n        ordering = sorted(range(n_clusters), key=lambda k: avg[k])\n        summary[i] = ' '.join([emails[i][closest[idx]] for idx in ordering])\n    print('Clustering Finished')\n    return summary\n      \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conda install numpy scipy mkl <nose> <sphinx> <pydot-ng>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install parameterized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conda install theano pygpu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install <--user> Theano[test, doc]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r geniusai-research/email-summarization/blob/master/requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk; nltk.download(\"punkt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"md5sum skip-thoughts/models/*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summarize(df.content_clean)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}